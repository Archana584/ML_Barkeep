{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ML Barkeep TextGenRNN code",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackyhoover/ML_Barkeep/blob/master/ML_Barkeep_TextGenRNN_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H7LoMj4GA4n_",
        "colab_type": "text"
      },
      "source": [
        "#  Interactive textgenrnn Demo w/ GPU\n",
        "\n",
        "by [Max Woolf](http://minimaxir.com)\n",
        "\n",
        "*Last updated: December 2nd, 2018*\n",
        "\n",
        "Generate text using a pretrained neural network with a few lines of code, or easily train your own text-generating neural network of any size and complexity, **for free on a GPU using Collaboratory!**\n",
        "\n",
        "For more about textgenrnn, you can visit [this GitHub repository](https://github.com/minimaxir/textgenrnn).\n",
        "\n",
        "\n",
        "To get started:\n",
        "\n",
        "1. Copy this notebook to your Google Drive to keep it and save your changes.\n",
        "2. Make sure you're running the notebook in Google Chrome.\n",
        "3. Run the cells below:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KBkpRgBCBS2_",
        "colab_type": "code",
        "outputId": "fd0c03ca-4b42-41f1-c484-0ee34003c045",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "!pip install -q textgenrnn\n",
        "from google.colab import files\n",
        "from textgenrnn import textgenrnn\n",
        "from datetime import datetime\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0wXB05bPDYxS",
        "colab_type": "text"
      },
      "source": [
        "Set the textgenrnn model configuration here: the default parameters here give good results for most workflows. (see the [demo notebook](https://github.com/minimaxir/textgenrnn/blob/master/docs/textgenrnn-demo.ipynb) for more information about these parameters)\n",
        "\n",
        "If you are using an input file where documents are line-delimited, make sure to set `line_delimited` to `True`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P8wSlgXoDPCR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_cfg = {\n",
        "    'word_level': False,   # set to True if want to train a word-level model (requires more data and smaller max_length)\n",
        "    'rnn_size':128,   # number of LSTM cells of each layer (128/256 recommended)\n",
        "    'rnn_layers': 3,   # number of LSTM layers (>=2 recommended)\n",
        "    'rnn_bidirectional': False,   # consider text both forwards and backward, can give a training boost\n",
        "    'max_length': 30,   # number of tokens to consider before predicting the next (20-40 for characters, 5-10 for words recommended)\n",
        "    'max_words': 100000,   # maximum number of words to model; the rest will be ignored (word-level model only)\n",
        "}\n",
        "\n",
        "train_cfg = {\n",
        "    'line_delimited': True,   # set to True if each text has its own line in the source file\n",
        "    'num_epochs': 70,   # set higher to train the model for longer\n",
        "    'gen_epochs': 10,   # generates sample text from model after given number of epochs\n",
        "    'train_size': 0.8,   # proportion of input data to train on: setting < 1.0 limits model from learning perfectly\n",
        "    'dropout': 0.0,   # ignore a random proportion of source tokens each epoch, allowing model to generalize better\n",
        "    'validation': True,   # If train__size < 1.0, test on holdout dataset; will make overall training slower\n",
        "    'is_csv': False   # set to True if file is a CSV exported from Excel/BigQuery/pandas\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BT__brhBCvJu",
        "colab_type": "text"
      },
      "source": [
        "In the Colaboratory Notebook sidebar on the left of the screen, select *Files*. From there you can upload files:\n",
        "\n",
        "![alt text](https://i.imgur.com/TGcZT4h.png)\n",
        "\n",
        "Upload **any text file** and update the file name in the cell below, then run the cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i8z-r-zISmw5",
        "colab_type": "code",
        "outputId": "3b885b3b-e2a1-4c1f-9783-27471cd7350f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6OFnPCLADfll",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "file_name = \"/content/drive/My Drive/cocktails2.txt\"\n",
        "model_name = 'colaboratory'   # change to set file name of resulting trained models/texts"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LdpZQXknFNY3",
        "colab_type": "text"
      },
      "source": [
        "The next cell will start the actual training. And thanks to the power of Keras's CuDNN layers, training is super-fast when compared to CPU training on a local machine!\n",
        "\n",
        "Ideally, you want a training loss less than `1.0` in order for the model to create sensible text consistently."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aeXshJM-Cuaf",
        "colab_type": "code",
        "outputId": "33418f9a-3d9c-4b11-9226-d4752c024ae7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "textgen = textgenrnn(name=model_name)\n",
        "\n",
        "train_function = textgen.train_from_file if train_cfg['line_delimited'] else textgen.train_from_largetext_file\n",
        "\n",
        "train_function(\n",
        "    file_path=file_name,\n",
        "    new_model=True,\n",
        "    num_epochs=train_cfg['num_epochs'],\n",
        "    gen_epochs=train_cfg['gen_epochs'],\n",
        "    batch_size=1024,\n",
        "    train_size=train_cfg['train_size'],\n",
        "    dropout=train_cfg['dropout'],\n",
        "    validation=train_cfg['validation'],\n",
        "    is_csv=train_cfg['is_csv'],\n",
        "    rnn_layers=model_cfg['rnn_layers'],\n",
        "    rnn_size=model_cfg['rnn_size'],\n",
        "    rnn_bidirectional=model_cfg['rnn_bidirectional'],\n",
        "    max_length=model_cfg['max_length'],\n",
        "    dim_embeddings=100,\n",
        "    word_level=model_cfg['word_level'])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:541: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4432: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:203: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "479 texts collected.\n",
            "Training new model w/ 3-layer, 128-cell LSTMs\n",
            "Training on 37,193 character sequences.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "Epoch 1/70\n",
            "36/36 [==============================] - 7s 200ms/step - loss: 3.3907 - val_loss: 2.9773\n",
            "Epoch 2/70\n",
            "36/36 [==============================] - 3s 76ms/step - loss: 2.9540 - val_loss: 2.9738\n",
            "Epoch 3/70\n",
            "36/36 [==============================] - 3s 92ms/step - loss: 2.9433 - val_loss: 2.9510\n",
            "Epoch 4/70\n",
            "36/36 [==============================] - 3s 89ms/step - loss: 2.9216 - val_loss: 2.9256\n",
            "Epoch 5/70\n",
            "36/36 [==============================] - 3s 87ms/step - loss: 2.8390 - val_loss: 2.8555\n",
            "Epoch 6/70\n",
            "36/36 [==============================] - 3s 89ms/step - loss: 2.7846 - val_loss: 2.7800\n",
            "Epoch 7/70\n",
            "36/36 [==============================] - 3s 90ms/step - loss: 2.6735 - val_loss: 2.7701\n",
            "Epoch 8/70\n",
            "36/36 [==============================] - 3s 86ms/step - loss: 2.4439 - val_loss: 2.3073\n",
            "Epoch 9/70\n",
            "36/36 [==============================] - 3s 88ms/step - loss: 2.0637 - val_loss: 1.9688\n",
            "Epoch 10/70\n",
            "36/36 [==============================] - 3s 87ms/step - loss: 1.9625 - val_loss: 1.8110\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "Cooooka glass 1 2 oz  Lme                              \n",
            "\n",
            "Coooot glass 1 1/2 oz  Lime                   2          \n",
            "\n",
            "Cooool glass 1 1/2  Gne 1  Lie                              \n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "Ckalingain glass 1/2 oz  1 oz  Lmm oz  Gian glan 1 Lme Cruer 1 Berine                            e   \n",
            "\n",
            "Clocaine glas 1/2  Gain                        \n",
            "\n",
            "Cooon glas 1/2 oz  Khale  Berrcer 1 2  Biras 1 oz  Lmoo 1 Hblae                 \n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Clokatass 2  gla 3o  Iirppe 4ot  Lniueoe 1 Buueay jucce 1o 1/Lhet  Lgin LmhWm                \n",
            "\n",
            "-Cckadigle glass 2 ozo Cethrn 1/oz  Lim Cnt  Serepnuje cluCe 1 2 c26cry 4o ofoz 1 oz  15 z piofe rmah  Yuaery 4 cofl juuc Jcuuah  Anmn Culore \n",
            "\n",
            "anCr glash 1 1 oz oz prt Khl niut 2 cyplr3p la clpin1j 1  Lien 1/2 oz  Ktahotts  Flhil 1 oz  Grierrry 2 owCe  rum oz oz  Drr oz  Mla alo 1/2  1/2 crue 10o5  1t sä rere Amour Suwitt Klhkry      \n",
            "\n",
            "Epoch 11/70\n",
            "36/36 [==============================] - 2s 51ms/step - loss: 1.7197 - val_loss: 1.6667\n",
            "Epoch 12/70\n",
            "36/36 [==============================] - 3s 89ms/step - loss: 1.5831 - val_loss: 1.5724\n",
            "Epoch 13/70\n",
            "36/36 [==============================] - 3s 89ms/step - loss: 1.4862 - val_loss: 1.4820\n",
            "Epoch 14/70\n",
            "36/36 [==============================] - 3s 89ms/step - loss: 1.3915 - val_loss: 1.3700\n",
            "Epoch 15/70\n",
            "36/36 [==============================] - 3s 89ms/step - loss: 1.2925 - val_loss: 1.3125\n",
            "Epoch 16/70\n",
            "36/36 [==============================] - 3s 90ms/step - loss: 1.3614 - val_loss: 1.2385\n",
            "Epoch 17/70\n",
            "36/36 [==============================] - 3s 90ms/step - loss: 1.1585 - val_loss: 1.1951\n",
            "Epoch 18/70\n",
            "36/36 [==============================] - 3s 89ms/step - loss: 1.0959 - val_loss: 1.1575\n",
            "Epoch 19/70\n",
            "36/36 [==============================] - 3s 88ms/step - loss: 1.0739 - val_loss: 1.0754\n",
            "Epoch 20/70\n",
            "36/36 [==============================] - 3s 88ms/step - loss: 0.9798 - val_loss: 1.0222\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "Collins Glass 1 1/2 oz  Gin 1 tsp  Powdered sugar 1 oz  Gin 1 tsp  Corfeer Corurine 1 oz  Gin 1 tsp  Corurice 1 tsp  Coruce 1 oz  Corurined 1 oz  Gin 1/2 oz  Gin 1 oz  Gin 1/2 oz  Gin 1 tsp  Powdered sugar 1 oz  Gin 1 tsp  Sugar 1 oz  Gin 1/2 oz  Gin 1 tsp  Powdered sugar 1 oz  Corubon 1 tsp  Powi\n",
            "\n",
            "Cocktail glass 1 1/2 oz  Gin 1/2 oz  Gin 1 tsp  Powdered sugar 1 shot  Gin 1/2 oz  Corurined witer                   \n",
            "\n",
            "Colckail glass 1 1/2 oz  Gin 1 tsp  Sugar 1 oz  Corurberry 1 oz  Gin 1 tsp  Corruite 1 oz  Gin 1 tsp  Powdered sugar 1 oz  Sweete Cream 1 tsp  Sweeed sugar 1 oz  Cranberry juice                 \n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "Cocktail glass 1 oz  Vodka 1 oz  Gin 1 oz  Kaalar                 \n",
            "\n",
            "Cocktail glass 2 oz  Gin                   \n",
            "\n",
            "Cocktail glass 1 parts  Andise 1 oz  Terine cerron                 \n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Perin ol glass 1 1/2 oz  Toicotter                   \n",
            "\n",
            "Puitho Class 1 1/2 oz  Vodka of Boluter schbertocouut bocot rume 1/2 twst  Gin 2 spol \n",
            "\n",
            "Cofeninongleds Ireschonte Beterd  Lighter               \n",
            "\n",
            "Epoch 21/70\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 0.9221 - val_loss: 0.9634\n",
            "Epoch 22/70\n",
            "36/36 [==============================] - 3s 90ms/step - loss: 0.8701 - val_loss: 0.9196\n",
            "Epoch 23/70\n",
            "36/36 [==============================] - 3s 89ms/step - loss: 0.8186 - val_loss: 0.9397\n",
            "Epoch 24/70\n",
            "36/36 [==============================] - 3s 89ms/step - loss: 0.7742 - val_loss: 0.8656\n",
            "Epoch 25/70\n",
            "36/36 [==============================] - 3s 88ms/step - loss: 0.7279 - val_loss: 0.8700\n",
            "Epoch 26/70\n",
            "36/36 [==============================] - 3s 90ms/step - loss: 0.6923 - val_loss: 0.7952\n",
            "Epoch 27/70\n",
            "36/36 [==============================] - 3s 89ms/step - loss: 0.6598 - val_loss: 0.7779\n",
            "Epoch 28/70\n",
            "36/36 [==============================] - 3s 90ms/step - loss: 0.6169 - val_loss: 0.7751\n",
            "Epoch 29/70\n",
            "36/36 [==============================] - 3s 89ms/step - loss: 0.5932 - val_loss: 0.7483\n",
            "Epoch 30/70\n",
            "36/36 [==============================] - 3s 91ms/step - loss: 0.5628 - val_loss: 0.7352\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "Collins Glass 1 oz  Gin 1/2 oz  Light rum 1 oz  Triple sec 1 oz  Lemon juice 1 tsp  Powdered sugar 1 shot  Creme de Cassis 1 oz  Butter 1 cup  Vodka 1 oz  Vodka 1 oz  Gin 1/2 oz  Triple sec 1 oz  Cranberry juice                 \n",
            "\n",
            "Collins Glass 1 oz  Vodka 1 oz  Coca-Cola                   \n",
            "\n",
            "Collins glass 1 oz  Gin 1/2 oz  Gin 1/2 oz  Triple sec 1 oz  Coconut bott water \n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "Cocktail glass 1/2 oz  Vodka 1 oz  Sweet Vermouth 1 tsp  Water 1 cup  Mailedic 1/2 oz  Vodka 1 oz  Triple sec 1 oz  Vodka 1 oz  Vodka 2 oz  Taspers 1 shot  Sour mix                 \n",
            "\n",
            "Collins Glass 2 oz  Vodka 2 oz  Coffee 1/2 oz  Gin 1/2 oz  Bailey's irish cream 2 dashes  Angest schnapps 1 oz  Light rum 1 oz  Amaretto 1/2 oz  Vodka 1 oz  Bangerus 3/4 oz  Triple sec 1 shot  Vodka 1 oz  Vodka 2 oz  Vodka 1/2 oz  Dry Vermouth 1/2 oz  Amaretto 1/2 oz  Light rum 1 oz  Bourbon 1 oz \n",
            "\n",
            "Collins Glass 2 oz  Vodka 2 oz  Coffee 1 oz  Vodka 3 oz  Orange juice               \n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Highball glass 750 oz  Gallo cherry              \n",
            "\n",
            "Collins glass 3/3 oz  Zima-Colao 2 cups  Water               \n",
            "\n",
            "Proingl glass 1 oz  Absolut Coca-Colo al with  Orange juice 1 tsp leron Teon 3 oz  Temon juice 1 dashes  Strapberde 1/2 cup  Water 1/2 cup Sugar  Ice               \n",
            "\n",
            "Epoch 31/70\n",
            "36/36 [==============================] - 2s 54ms/step - loss: 0.5378 - val_loss: 0.7408\n",
            "Epoch 32/70\n",
            "36/36 [==============================] - 3s 90ms/step - loss: 0.5179 - val_loss: 0.7206\n",
            "Epoch 33/70\n",
            "36/36 [==============================] - 3s 91ms/step - loss: 0.4920 - val_loss: 0.7112\n",
            "Epoch 34/70\n",
            "36/36 [==============================] - 3s 91ms/step - loss: 0.4675 - val_loss: 0.7155\n",
            "Epoch 35/70\n",
            "36/36 [==============================] - 3s 90ms/step - loss: 0.4514 - val_loss: 0.7203\n",
            "Epoch 36/70\n",
            "36/36 [==============================] - 3s 90ms/step - loss: 0.4341 - val_loss: 0.7054\n",
            "Epoch 37/70\n",
            "36/36 [==============================] - 3s 89ms/step - loss: 0.4138 - val_loss: 0.7060\n",
            "Epoch 38/70\n",
            "36/36 [==============================] - 3s 91ms/step - loss: 0.3986 - val_loss: 0.7250\n",
            "Epoch 39/70\n",
            "36/36 [==============================] - 3s 91ms/step - loss: 0.3828 - val_loss: 0.7138\n",
            "Epoch 40/70\n",
            "36/36 [==============================] - 3s 92ms/step - loss: 0.3651 - val_loss: 0.7185\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "Cocktail glass 1 1/2 oz  Absolut Citron 1/4 oz  Southern Comfort 1 oz  Brandy 1 oz  Light rum 1 oz  Pineapple juice 1 tsp  Lime juice 1 tsp  Powdered sugar 1 whole  Egg 1 oz  Lemon juice 1 tsp superfine  Sugar 1 1/2 oz  Lemon juice 1 oz  Sweet Vermouth 1 oz  Cranberry juice               \n",
            "\n",
            "Cocktail glass 1 1/2 oz  Gin 1/2 oz  Sweet Vermouth 1 oz  Cranberry juice               \n",
            "\n",
            "Cocktail glass 1 1/2 oz  Gin 1/2 oz  Lemon juice 1 oz  Sweet Vermouth 1 oz  Pineapple juice 1 part  Cranberry juice               \n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "Cocktail glass 1 1/2 oz  Gin 1 tsp  Sugar 1 Maraschino cherry 1 Orange           \n",
            "\n",
            "Cocktail glass 1 oz  Apple juice 1 oz  Cranberry juice               \n",
            "\n",
            "Highball glass 2 shots  Vodka 1 shot  Sweet Vermouth 1 oz  Dry Vermouth 1 oz  Cranberry juice               \n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Highball glass 1 part  Saurt brandy 1 tsp  Grenadine 1 jigger  Gin 1 tsp  Maraschino liquuer 1 part  Bitters 1 cup  Chrinborame deron 3 cupswes 1 oz  Vockaot Beat 1 oz  Orange juice               \n",
            "\n",
            "Wine Malast coutherrin  Sugar 1 garua Bourbon                  \n",
            "\n",
            "Cocktail glass 3 oz  Rumk 2 oz  750. Grnaiju 1 shot  Crenbe Creame Evedst rim \n",
            "\n",
            "Epoch 41/70\n",
            "36/36 [==============================] - 2s 53ms/step - loss: 0.3545 - val_loss: 0.7345\n",
            "Epoch 42/70\n",
            "36/36 [==============================] - 3s 90ms/step - loss: 0.3383 - val_loss: 0.7333\n",
            "Epoch 43/70\n",
            "36/36 [==============================] - 3s 92ms/step - loss: 0.3224 - val_loss: 0.7495\n",
            "Epoch 44/70\n",
            "36/36 [==============================] - 3s 92ms/step - loss: 0.3168 - val_loss: 0.7300\n",
            "Epoch 45/70\n",
            "36/36 [==============================] - 3s 92ms/step - loss: 0.2990 - val_loss: 0.7435\n",
            "Epoch 46/70\n",
            "36/36 [==============================] - 3s 90ms/step - loss: 0.2949 - val_loss: 0.7318\n",
            "Epoch 47/70\n",
            "36/36 [==============================] - 3s 90ms/step - loss: 0.2776 - val_loss: 0.7581\n",
            "Epoch 48/70\n",
            "36/36 [==============================] - 3s 89ms/step - loss: 0.2639 - val_loss: 0.8543\n",
            "Epoch 49/70\n",
            "36/36 [==============================] - 3s 88ms/step - loss: 0.2672 - val_loss: 0.7613\n",
            "Epoch 50/70\n",
            "36/36 [==============================] - 3s 92ms/step - loss: 0.2437 - val_loss: 0.7654\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "Cocktail glass 1 oz  Dry Vermouth 1 oz  Pineapple juice                 \n",
            "\n",
            "Cocktail glass 1 oz  Dry Vermouth 1 oz  Pineapple juice                 \n",
            "\n",
            "Cocktail glass 1 oz  Brandy 1 oz  Lime juice 1/2 tsp  Grenadine 1/2 oz  Light rum 1/2 oz  Tequila 1/2 oz  Triple sec 1 oz  Lemon juice                 \n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "Cocktail glass 1 1/2 oz  Gin 1/2 oz  Sweet Vermouth 1/2 oz  Gin 1/2 oz  Sweet Vermouth 1 oz  Dry Vermouth 1 twist of  Lemon peel                 \n",
            "\n",
            "Cocktail glass 1 oz  Jägermeister 1/2 oz  Vodka 1/2 oz  Cranberry juice                 \n",
            "\n",
            "Cocktail glass 2 oz  Gin 1/2 oz  Triple sec 1 oz  Coffee Cranbery 2 dashes  Orange juice 1 gal  Coca-Coola Syrup 1/2 cup  Cream of 1/2 oz  Lime juice 1 tsp  Sugar 1 Cherry 1 cup  Water 1 oz  Sweet and sour 1 wedle  Lemon  cubes Ice Cohnat \n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Coffee Mug 1 oz  Kahlut Kurant Fruica 1 dash  Coca-Cola                  \n",
            "\n",
            "Collins Glass 2 oz spice  Coda-Ap                \n",
            "\n",
            "Collins glass 1 shot  Dark rum 1 slice  Lemon 1 Cherry           \n",
            "\n",
            "Epoch 51/70\n",
            "36/36 [==============================] - 2s 54ms/step - loss: 0.2462 - val_loss: 0.7897\n",
            "Epoch 52/70\n",
            "36/36 [==============================] - 3s 90ms/step - loss: 0.2441 - val_loss: 0.7772\n",
            "Epoch 53/70\n",
            "36/36 [==============================] - 3s 89ms/step - loss: 0.2237 - val_loss: 0.7835\n",
            "Epoch 54/70\n",
            "36/36 [==============================] - 3s 89ms/step - loss: 0.2155 - val_loss: 0.7898\n",
            "Epoch 55/70\n",
            "36/36 [==============================] - 3s 90ms/step - loss: 0.2075 - val_loss: 0.8016\n",
            "Epoch 56/70\n",
            "36/36 [==============================] - 3s 92ms/step - loss: 0.2008 - val_loss: 0.8055\n",
            "Epoch 57/70\n",
            "36/36 [==============================] - 3s 88ms/step - loss: 0.1950 - val_loss: 0.8123\n",
            "Epoch 58/70\n",
            "36/36 [==============================] - 3s 90ms/step - loss: 0.1899 - val_loss: 0.8070\n",
            "Epoch 59/70\n",
            "36/36 [==============================] - 3s 90ms/step - loss: 0.1829 - val_loss: 0.8246\n",
            "Epoch 60/70\n",
            "36/36 [==============================] - 3s 89ms/step - loss: 0.1775 - val_loss: 0.8223\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "Cocktail glass 1 oz  Brandy 1 oz white  Creme de Cacao 1/2 oz  Light cream 1 tsp  Water 1 tsp  Anisett Carbonated water \n",
            "\n",
            "Cocktail glass 1 1/2 oz  Gin 1/2 oz  Light rum 1/2 oz  Triple sec 1 oz  Chambord raspberry liqueur 1 oz  Cranberry juice               \n",
            "\n",
            "Cocktail glass 1 1/2 oz  Gin 1/2 oz  Light rum 1 tblsp  Triple sec 1 Banana 1 1/2 tsp  Triple sec 1 part  Milk 1/2 oz  Triple sec 1 tblsp  Lemon juice                 \n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "Collins glass 1 shot  Dark rum 1 slice  Lemon 1 Chermed  Coca-Cola                  \n",
            "\n",
            "Cocktail glass 1 1/2 oz  Gin 1 tsp  Benedictine                  \n",
            "\n",
            "Cocktail glass 2 oz  Gin 1/2 oz  Triple sec 1/2 oz  Tequila 1/2 oz  Midori melon liqueur 1/2 oz  Peach schnapps                   \n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Whiskey sou glaas 2 oz  Orange juice 2 oz  Ginger ale 1 oz  Campap beavase 1/2 oz  Tequila Aboutte Carbonated water Tange\n",
            "\n",
            "Old-fashioned glass 1/2 shot  Milk 3 oz  Cranberry juice 2 oz  Ginger ale 1 twist of  Lemon peel         \n",
            "\n",
            "Cocktail glass 1 1/2 oz  Tequila 1/2 oz  Midori melon liqueur 1/2 oz  Sweet and sour 1 dash  Grenadine               \n",
            "\n",
            "Epoch 61/70\n",
            "36/36 [==============================] - 2s 52ms/step - loss: 0.1725 - val_loss: 0.8297\n",
            "Epoch 62/70\n",
            "36/36 [==============================] - 3s 90ms/step - loss: 0.1671 - val_loss: 0.8320\n",
            "Epoch 63/70\n",
            "36/36 [==============================] - 3s 90ms/step - loss: 0.1629 - val_loss: 0.8418\n",
            "Epoch 64/70\n",
            "36/36 [==============================] - 3s 92ms/step - loss: 0.1593 - val_loss: 0.8417\n",
            "Epoch 65/70\n",
            "36/36 [==============================] - 3s 91ms/step - loss: 0.1550 - val_loss: 0.8493\n",
            "Epoch 66/70\n",
            "36/36 [==============================] - 3s 91ms/step - loss: 0.1515 - val_loss: 0.8527\n",
            "Epoch 67/70\n",
            "36/36 [==============================] - 3s 90ms/step - loss: 0.1484 - val_loss: 0.8514\n",
            "Epoch 68/70\n",
            "36/36 [==============================] - 3s 91ms/step - loss: 0.1456 - val_loss: 0.8570\n",
            "Epoch 69/70\n",
            "36/36 [==============================] - 3s 90ms/step - loss: 0.1432 - val_loss: 0.8542\n",
            "Epoch 70/70\n",
            "36/36 [==============================] - 3s 92ms/step - loss: 0.1410 - val_loss: 0.8577\n",
            "####################\n",
            "Temperature: 0.2\n",
            "####################\n",
            "Cocktail glass 1 oz  Brandy 1 oz  Amaretto 1 oz  Light cream                 \n",
            "\n",
            "Cocktail glass 1 1/2 oz  Gin 1 oz  Triple sec 1 tsp  Lemon juice 1 twist of  Lemon peel               \n",
            "\n",
            "Collins Glass 1 oz  Rum 1 oz  Vodka 1 oz  Coffee liqueur Light cream \n",
            "\n",
            "####################\n",
            "Temperature: 0.5\n",
            "####################\n",
            "Cocktail glass 1 1/2 oz  Dry Vermouth 3 dashes  Bitters               \n",
            "\n",
            "Cocktail glass 1 1/2 oz  Blended whiskey Juice of 1/2  Lemon 1 tsp  Powdered sugar 1 whole  Egg Nutmeg \n",
            "\n",
            "Collins glass 2 oz  Light rum 1 oz  Lemon juice                 \n",
            "\n",
            "####################\n",
            "Temperature: 1.0\n",
            "####################\n",
            "Old-fashioned glass 1 shot  Bacardi Limon                   \n",
            "\n",
            "Cocktail glass 2 1/2 oz  Cherry brandy 1 1/2 oz  Gin 2 oz  Sweet and sour 1 whole  Egg rot                \n",
            "\n",
            "Cocktail glass 1 1/2 oz  Lemon vodka 1 oz  Dry Vermouth 1 cup  Cranberry juice               \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RTa6zf3e_9gV",
        "colab_type": "text"
      },
      "source": [
        "You can download a large amount of generated text from your model with the cell below! Rerun the cell as many times as you want for even more text!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fxL77nvAMAX",
        "colab_type": "code",
        "outputId": "20794ef8-f05c-48df-8018-913aac1bb87e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "# this temperature schedule cycles between 1 very unexpected token, 1 unexpected token, 2 expected tokens, repeat.\n",
        "# changing the temperature schedule can result in wildly different output!\n",
        "temperature = [1.0, 0.5, 0.2, 0.2]   \n",
        "prefix = None   # if you want each generated text to start with a given seed text\n",
        "\n",
        "if train_cfg['line_delimited']:\n",
        "  n = 1000\n",
        "  max_gen_length = 60 if model_cfg['word_level'] else 300\n",
        "else:\n",
        "  n = 1\n",
        "  max_gen_length = 2000 if model_cfg['word_level'] else 10000\n",
        "  \n",
        "timestring = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "gen_file = '{}_gentext_{}.txt'.format(model_name, timestring)\n",
        "\n",
        "textgen.generate_to_file(gen_file,\n",
        "                         temperature=temperature,\n",
        "                         prefix=prefix,\n",
        "                         n=n,\n",
        "                         max_gen_length=max_gen_length)\n",
        "files.download(gen_file)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-d9665c83b453>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m                          \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprefix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                          \u001b[0mn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m                          max_gen_length=max_gen_length)\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/textgenrnn/textgenrnn.py\u001b[0m in \u001b[0;36mgenerate_to_file\u001b[0;34m(self, destination_path, **kwargs)\u001b[0m\n\u001b[1;32m    335\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgenerate_to_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestination_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 337\u001b[0;31m         \u001b[0mtexts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_as_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    338\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdestination_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    339\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/textgenrnn/textgenrnn.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, n, return_as_list, prefix, temperature, max_gen_length, interactive, top_n)\u001b[0m\n\u001b[1;32m     85\u001b[0m                                            \u001b[0mmax_gen_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                                            \u001b[0minteractive\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m                                            top_n)\n\u001b[0m\u001b[1;32m     88\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mreturn_as_list\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{}\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/textgenrnn/utils.py\u001b[0m in \u001b[0;36mtextgenrnn_generate\u001b[0;34m(model, vocab, indices_char, prefix, temperature, maxlen, meta_token, word_level, single_text, max_gen_length, interactive, top_n)\u001b[0m\n\u001b[1;32m     89\u001b[0m             \u001b[0;31m# auto-generate text without user intervention\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             next_index = textgenrnn_sample(\n\u001b[0;32m---> 91\u001b[0;31m                 \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoded_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m                 next_temperature)\n\u001b[1;32m     93\u001b[0m             \u001b[0mnext_char\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_char\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1399\u001b[0m                                             \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1400\u001b[0m                                             \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1401\u001b[0;31m                                             callbacks=callbacks)\n\u001b[0m\u001b[1;32m   1402\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1403\u001b[0m     def train_on_batch(self, x, y,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mpredict_loop\u001b[0;34m(model, f, ins, batch_size, verbose, steps, callbacks)\u001b[0m\n\u001b[1;32m    330\u001b[0m             \u001b[0mbatch_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'batch'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'size'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m             \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_batch_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'predict'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'begin'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m             \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbatch_index\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2977\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2979\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2980\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2981\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2935\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2937\u001b[0;31m             \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2938\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1456\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1457\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1458\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1459\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1460\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClJwpF_ACONp",
        "colab_type": "text"
      },
      "source": [
        "You can download the weights and configuration files in the cell below, allowing you recreate the model on your own computer!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4RNY6RBI9LmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "files.download('{}_weights.hdf5'.format(model_name))\n",
        "files.download('{}_vocab.json'.format(model_name))\n",
        "files.download('{}_config.json'.format(model_name))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oF4-PqF0Fl7R",
        "colab_type": "text"
      },
      "source": [
        "To recreate the model on your own computer, after installing textgenrnn and TensorFlow, you can create a Python script with:\n",
        "\n",
        "```\n",
        "from textgenrnn import textgenrnn\n",
        "textgen = textgenrnn(weights_path='colaboratory_weights.hdf5',\n",
        "                       vocab_path='colaboratory_vocab.json',\n",
        "                       config_path='colaboratory_config.json')\n",
        "                       \n",
        "textgen.generate_samples(max_gen_length=1000)\n",
        "textgen.generate_to_file('textgenrnn_texts.txt', max_gen_length=1000)\n",
        "```\n",
        "\n",
        "Have fun with your new model! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92Zjtsb_Dgj-",
        "colab_type": "text"
      },
      "source": [
        "# Etcetera\n",
        "\n",
        "If the model fails to load on a local machine due to a model-size-not-matching bug (common in >30MB weights), this is due to a file export bug from Colaboratory. To work around this issue, save the weights to Google Drive with the two cells below and download from there."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-IzscxUHmAB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from google.colab import files\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WR4_XJpfKAIn",
        "colab_type": "code",
        "outputId": "f70ce499-1062-4968-8423-c7dbcd71cc5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "uploaded = drive.CreateFile({'title': '{}_weights.hdf5'.format(model_name)})\n",
        "uploaded.SetContentFile('{}_weights.hdf5'.format(model_name))\n",
        "uploaded.Upload()\n",
        "print('Uploaded file with ID {}'.format(uploaded.get('id')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Uploaded file with ID 1b6T6M32YnXs-c0NB-PEi6MhAdCuG7RHy\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ig-KVgkCDCKD",
        "colab_type": "text"
      },
      "source": [
        "If the notebook has errors (e.g. GPU Sync Fail), force-kill the Colaboratory virtual machine and restart it with the command below:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rIHiVP53FnsX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}